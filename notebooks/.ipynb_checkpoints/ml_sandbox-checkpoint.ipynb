{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://logodownload.org/wp-content/uploads/2016/03/premier-league-5.png\" width=\"75\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n",
    "\n",
    "# Premier League API Retrieval and Database Building: Create & Access SQLite database using Python\n",
    "\n",
    "<!-- Estimated time needed: **15** minutes -->\n",
    "\n",
    "## Functions of notebook\n",
    "\n",
    "This notebook was build to:\n",
    "\n",
    "*   Retrieve data from Premier League API\n",
    "-   Create a database from data\n",
    "*   Insert retrieved data into database\n",
    "*   Query data from the table to build visualization and statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn numpy pandas matplotlib --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.functions.api_operations import FPLAPIParser\n",
    "from src.functions.raw_data_compiler import RawDataCompiler\n",
    "from src.functions.data_processing import DataAnalytics\n",
    "from src.functions.helper_fns import GeneralHelperFns\n",
    "from src.functions.notebook_operations import VisualizationOperations\n",
    "\n",
    "api_ops = FPLAPIParser()\n",
    "data_compiler = RawDataCompiler(api_ops)\n",
    "helper_fns = GeneralHelperFns(api_ops, data_compiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_analytics = DataAnalytics(api_ops, data_compiler, helper_fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functions.helper_fns import UnderStatHelperFns\n",
    "from src.functions.understat_operations import UnderstatProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understat_helper_fns = UnderStatHelperFns(api_ops, data_compiler)\n",
    "understat_ops = UnderstatProcessing(api_ops, data_analytics, helper_fns, understat_helper_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understat_ops.tabulate_ratings_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(understat_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data = understat_ops.fetch_all_team_expanded_stats()\n",
    "new_data = []\n",
    "for d in team_data:\n",
    "    new_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, list) and all(isinstance(item, dict) for item in value):\n",
    "            for item in value:\n",
    "                for sub_key, sub_value in item.items():\n",
    "                    new_dict_key = f\"{key}_{sub_key}\"\n",
    "                    if new_dict_key not in new_dict:\n",
    "                        new_dict[new_dict_key] = []\n",
    "                    new_dict[new_dict_key].append(sub_value)\n",
    "        else:\n",
    "            new_dict[key] = value\n",
    "    new_data.append(new_dict)\n",
    "\n",
    "team_dict = {d['id']: d for d in new_data}\n",
    "team_dict = dict(sorted(team_dict.items(), key=lambda x: x[1]['title']))\n",
    "\n",
    "team_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df = pd.DataFrame(team_data[12])\n",
    "\n",
    "def extract_values(row, key):\n",
    "    return row[key]\n",
    "\n",
    "for col in team_df.columns:\n",
    "    # Check if the column contains dictionaries\n",
    "    if all(isinstance(val, dict) for val in team_df[col]):\n",
    "        # Iterate over keys in dictionaries and create new columns\n",
    "        for key in team_df[col][0].keys():\n",
    "            new_col_name = f\"{col}_{key}\"\n",
    "            team_df[new_col_name] = team_df[col].apply(lambda x: x.get(key))\n",
    "        # Drop the original column\n",
    "        team_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "understat_helper_fns.team_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# understat = UnderstatClient()\n",
    "# player_shot_data = understat.player(player=str(understat_helper_fns.grab_player_USID_from_FPLID(308))).get_shot_data()\n",
    "# pd.DataFrame(data=player_shot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data = understat.league(league=\"EPL\").get_team_data(season=\"2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data['71']['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from understatapi import UnderstatClient\n",
    "\n",
    "# understat = UnderstatClient()\n",
    "# player_shot_data = understat.player(player=str(understat_helper_fns.grab_player_USID_from_FPLID(308))).get_shot_data()\n",
    "# player_shot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understat_helper_fns.grab_team_USname_from_FPLID(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming master ML data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Model on data you have*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing with raw element summary from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "ml_data = pd.DataFrame(data_compiler.master_summary_temp)\n",
    "# ml_data['team'] = ml_data['element'].apply(lambda x: helper_fns.grab_player_team_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging raw element summary with bootstrap for teams positions and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_cols_of_interest = ['id', 'team', 'element_type', 'first_name', 'second_name']\n",
    "bootstrap_df = pd.DataFrame(api_ops.raw_data['elements'])\n",
    "ml_data = pd.merge(ml_data, bootstrap_df[raw_data_cols_of_interest], left_on='element', right_on='id', how='left')\n",
    "ml_data.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only analyzing MIDs and FWDs (for this iteration), and taking players who played that game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_ml_data = ml_data.loc[ml_data['element_type'].isin([3,4])]\n",
    "# filtered_ml_data = filtered_ml_data[grab_fpl_stats_col_names(filtered_ml_data)]\n",
    "# filtered_ml_data = filtered_ml_data.loc[filtered_ml_data['minutes'] > 0].reset_index(drop=True)\n",
    "# filtered_ml_data = pd.get_dummies(data=filtered_ml_data, columns=['opponent_team', 'team'])\n",
    "# filtered_ml_data.replace([False, True], [0,1], inplace=True)\n",
    "# filtered_ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ml_data = ml_data.loc[ml_data['element_type'].isin([3,4])]\n",
    "filtered_ml_data = filtered_ml_data.loc[filtered_ml_data['minutes'] > 0].reset_index(drop=True)\n",
    "# filtered_ml_data = pd.get_dummies(data=filtered_ml_data, columns=['opponent_team', 'team'])\n",
    "filtered_ml_data.replace([False, True], [0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding understat team data per match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data = understat.league(league=\"EPL\").get_team_data(season=\"2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_understat_history(df):\n",
    "    fpl_api_kickoff_time_dt = datetime.fromisoformat(df['kickoff_time'].replace('Z', '+00:00')).strftime('%Y-%m-%d')\n",
    "    relevant_team_data_history_understat = team_data[str(understat_helper_fns.grab_team_USID_from_FPLID(df['team']))]['history']\n",
    "    return next((x for x in iter(relevant_team_data_history_understat) if datetime.strptime(x['date'], '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d') == fpl_api_kickoff_time_dt), None)\n",
    "\n",
    "updated_ml_data = filtered_ml_data.copy()\n",
    "prefix_id = 'team'\n",
    "for idx, row in tqdm_notebook(updated_ml_data.iterrows()):\n",
    "    result_dict = grab_understat_history(row)\n",
    "    if result_dict is not None:\n",
    "        for key, value in result_dict.items():\n",
    "            if key != 'date':\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        new_column_name = f\"{key}_{sub_key}\"\n",
    "                        if f'{prefix_id}_{new_column_name}' not in updated_ml_data.columns:\n",
    "                            updated_ml_data[f'{prefix_id}_{new_column_name}'] = None\n",
    "                        updated_ml_data.at[idx, f'{prefix_id}_{new_column_name}'] = sub_value\n",
    "                else:\n",
    "                    if f'{prefix_id}_{key}' not in updated_ml_data.columns:\n",
    "                        updated_ml_data[f'{prefix_id}_{key}'] = None\n",
    "                    updated_ml_data.at[idx, f'{prefix_id}_{key}'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ml_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "updated_ml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding averages, sums and std deviations for several team and player performance metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building column parser for direct grab of relevant information as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_fpl_stats_col_names(data: pd.DataFrame = ml_data, col_filter = None):\n",
    "#     remove_col_descripts = ['chance', 'cost', 'rank', 'start', 'name', 'dreamteam', 'news', 'photo','id', 'code', 'special', 'squad_number', 'percent', 'text', 'transfers', 'order','ep_', 'status', 'cards','element','missed','saved','minutes','own_goals', 'value']\n",
    "    remove_col_descripts = ['kickoff_time', 'red_cards', 'yellow_cards', 'missed', 'saved', 'name', 'transfers', 'element', 'fixture', 'selected', 'value', 'round', 'own_goals', 'score']\n",
    "    remove_col_descripts += ['saves', 'clean_sheets','conceded']\n",
    "    remove_col_descripts += ['team_result', 'team_wins','team_draws','team_loses','team_pts', 'team_h_a']\n",
    "    if col_filter is not None:\n",
    "        if col_filter == 'known': \n",
    "            remove_col_descripts += ['minutes', 'assists', 'bonus','bps', 'influence','creativity', 'threat','ict_index', 'starts', 'expected_goals', 'expected_assists', 'expected_goal_involvements']\n",
    "            remove_col_descripts += ['team_xG', 'team_xGA', 'team_npxG','team_npxGA', 'team_xpts']\n",
    "    return [x for x in data.keys() if all(y not in x for y in remove_col_descripts)]\n",
    "\n",
    "#     if position_filters is None:\n",
    "#         return [x for x in data.keys() if all(y not in x for y in remove_col_descripts)]\n",
    "#     else:\n",
    "#         filtered_elements = data.loc[data['element_type'].isin(position_filters)]\n",
    "#         if position_filters == [3,4]:\n",
    "#             remove_col_descripts += ['saves', 'clean_sheets','conceded']\n",
    "#             return [x for x in data.keys() if all(y not in x for y in remove_col_descripts)]\n",
    "#         else:\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ml_data[grab_fpl_stats_col_names(updated_ml_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming df_player contains player performance metrics and df_team contains team metrics\n",
    "# # Let's assume we want to analyze the correlation between player performance metrics and team metrics\n",
    "\n",
    "# # Select relevant columns\n",
    "# us_cols = ['team_xG', 'team_xGA', 'team_npxG', 'team_npxGA', 'team_ppda_att', 'team_ppda_def', 'team_ppda_allowed_att', 'team_ppda_allowed_def', 'team_deep', 'team_deep_allowed', 'team_xpts', 'team_npxGD']\n",
    "# player_cols = ['total_points', 'was_home', 'goals_scored', 'assists']\n",
    "\n",
    "# team_data = updated_ml_data[us_cols]\n",
    "# player_data = updated_ml_data[player_cols]\n",
    "\n",
    "# # Calculate correlation matrix\n",
    "# correlation_matrix = player_data.apply(lambda x: x.corr(team_data.mean(axis=1)))\n",
    "\n",
    "\n",
    "# # Visualize correlation matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "# plt.title('Correlation Matrix between Player and Team Metrics')\n",
    "# plt.xlabel('Team Metrics')\n",
    "# plt.ylabel('Player Metrics')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_param_to_label(param_name: str):\n",
    "    reference_data = api_ops.raw_data['element_stats']\n",
    "    if param_name in [x['name'] for x in reference_data]:\n",
    "        return next(x['label'] for x in reference_data if x['name'] == param_name)\n",
    "    else:\n",
    "        return ' '.join([word.capitalize() for word in re.split('[_-]', param_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_ops.raw_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# api_ops.raw_data['element_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# api_ops.raw_data['element_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_elements = pd.json_normalize(api_ops.raw_data['elements'])\n",
    "# raw_elements.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_players = pd.json_normalize(api_ops.raw_data['element_stats'])\n",
    "# raw_positions = pd.json_normalize(api_ops.raw_data['element_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slr(input_att: str, output_att: str, data: pd.DataFrame, naming_data = api_ops.raw_data['element_stats']):\n",
    "    \n",
    "    def df_with_numeric_values(param_cols_to_apply_condition, min_value, df):\n",
    "        return df[df[param_cols_to_apply_condition].apply(lambda x: x.ge(min_value) & x.apply(lambda y: isinstance(y, (int, float)) and not np.isnan(y)), axis=1)]\n",
    "    \n",
    "    lm = LinearRegression()\n",
    "\n",
    "    params_of_interest = [input_att, output_att]\n",
    "    reg_df = data[params_of_interest].astype('float')\n",
    "    reg_df = df_with_numeric_values(params_of_interest, 0, reg_df)\n",
    "\n",
    "    width = 6\n",
    "    height = 5\n",
    "\n",
    "    plt.figure(figsize=(width, height))\n",
    "    sns.regplot(x=input_att, y=output_att, data=reg_df, line_kws={\"color\": \"black\"})\n",
    "    plt.ylim(0,)\n",
    "    \n",
    "    param_name_x = convert_param_to_label(input_att)\n",
    "    param_name_y = convert_param_to_label(output_att)\n",
    "    plt.title(f'Simple Linear Regression for {param_name_y} vs {param_name_x}')\n",
    "    plt.xlabel(f'{param_name_x}')\n",
    "    plt.ylabel(f'{param_name_y}')\n",
    "\n",
    "    plt.figure(figsize=(width, height))\n",
    "    sns.residplot(x=reg_df[input_att], y=reg_df[output_att])\n",
    "    \n",
    "    plt.title(f'Residual Error for {param_name_y} vs {param_name_x}')\n",
    "    plt.xlabel(f'{param_name_x} (Indep. Variable)')\n",
    "    plt.ylabel(f'Residuals of SLR model')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab_fpl_stats_col_names([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slr(\"ict_index\", \"bps\", ml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def plot_mlr(input_atts: list, output_att: str, data: pd.DataFrame):\n",
    "    \n",
    "    def df_with_numeric_values(param_cols_to_apply_condition, min_value, df):\n",
    "        return df[df[param_cols_to_apply_condition].apply(lambda x: x.ge(min_value) & x.apply(lambda y: isinstance(y, (int, float)) and not np.isnan(y)), axis=1)]\n",
    "    \n",
    "    _naming_data = api_ops.raw_data['element_stats']\n",
    "    \n",
    "    lm = LinearRegression()\n",
    "\n",
    "    params_of_interest = [output_att]+input_atts\n",
    "    filtered_data = data[params_of_interest].copy().astype('float')\n",
    "    filtered_data = df_with_numeric_values(params_of_interest, 0.1, filtered_data).dropna()\n",
    "\n",
    "    output_df = filtered_data[output_att]\n",
    "    \n",
    "    Z = filtered_data[input_atts]\n",
    "    lm.fit(Z, filtered_data[output_att])\n",
    "    print(f\"LM Intercept: {lm.intercept_}\")\n",
    "    print(f\"LM Coefficients: {lm.coef_}\")\n",
    "    Y_hat = lm.predict(Z)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "\n",
    "    ax1 = sns.distplot(output_df, hist=False, color=\"r\", label=\"Actual Value\")\n",
    "    sns.distplot(Y_hat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n",
    "\n",
    "    if output_att in [x['name'] for x in _naming_data]:\n",
    "        param_name = next(x['label'] for x in _naming_data if x['name'] == output_att)\n",
    "    else:\n",
    "        param_name = result = ' '.join([word.capitalize() for word in re.split('[_-]', output_att)])\n",
    "\n",
    "    \n",
    "    plt.title(f'Distribution Plot of Actual vs Fitted Values for {param_name}')\n",
    "    plt.xlabel(f'{param_name}')\n",
    "    plt.ylabel(f'Proportion of {param_name}')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "# attributes = ['ict_index', 'expected_goal_involvements', 'bps']\n",
    "attributes = grab_fpl_stats_col_names(updated_ml_data, col_filter='known')\n",
    "\n",
    "# _position_data = api_ops.raw_data['element_types']\n",
    "# ml_data = ml_data.loc[ml_data['element_type'].isin([1,2])]\n",
    "plot_mlr(attributes, output_attribute, updated_ml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPolly(model, independent_variable, dependent_variable, param_names):\n",
    "    x_new = np.linspace(min(independent_variable), max(independent_variable), 100)\n",
    "    y_new = model(x_new)\n",
    "\n",
    "    plt.plot(independent_variable, dependent_variable, '.', x_new, y_new, '-')\n",
    "    plt.title(f'Polynomial Fit with Matplotlib for {param_names[1]}')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((0.898, 0.898, 0.898))\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    plt.xlabel(convert_param_to_label(param_names[0]))\n",
    "    plt.ylabel(convert_param_to_label(param_names[1]))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polyreg(input_att: str, output_att: str, data: pd.DataFrame, order: int):\n",
    "    \n",
    "    def df_with_numeric_values(param_cols_to_apply_condition, min_value, df):\n",
    "        return df[df[param_cols_to_apply_condition].apply(lambda x: x.ge(min_value) & x.apply(lambda y: isinstance(y, (int, float)) and not np.isnan(y)), axis=1)]\n",
    "    \n",
    "    params_of_interest = [input_att, output_att]\n",
    "    filtered_data = data[params_of_interest].copy().astype('float')\n",
    "    filtered_data = df_with_numeric_values(params_of_interest, 0.1, filtered_data).dropna()\n",
    "    \n",
    "    x = filtered_data[input_att]\n",
    "    y = filtered_data[output_att]\n",
    "\n",
    "    f = np.polyfit(x, y, order)\n",
    "    p = np.poly1d(f)\n",
    "    print(p)\n",
    "\n",
    "    PlotPolly(p, x, y, [input_att, output_att])\n",
    "\n",
    "    np.polyfit(x, y, order)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_att = 'ict_index'\n",
    "output_att = 'bps'\n",
    "\n",
    "plot_polyreg(input_att, output_att, filtered_ml_data, order = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_params = ['id','ict_index', 'expected_goal_involvements', 'event_points','bonus']\n",
    "# filtered_raw_elements = raw_elements[all_params].copy().astype('float')\n",
    "# filtered_raw_elements = df_with_numeric_values(all_params, 0.1, filtered_raw_elements).dropna()\n",
    "# df_group = filtered_raw_elements[all_params].astype('float')\n",
    "\n",
    "# params_of_interest = ['event_points','ict_index', 'expected_goal_involvements','bonus']\n",
    "# df_group = df_group.groupby(params_of_interest[1],as_index=False).mean()\n",
    "\n",
    "# grouped_pivot = df_group.pivot(index=params_of_interest[0],columns=params_of_interest[1])\n",
    "\n",
    "# width = 30\n",
    "# height = 11\n",
    "\n",
    "# plt.figure(figsize=(width, height))\n",
    "# plt.pcolor(grouped_pivot, cmap='RdBu')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "    \n",
    "    ax1 = sns.kdeplot(RedFunction, color=\"r\", label=RedName)\n",
    "    ax2 = sns.kdeplot(BlueFunction, color=\"b\", label=BlueName, ax=ax1)\n",
    "\n",
    "    plt.title(Title)\n",
    "    plt.xlabel('Distribution Parameter')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PollyPlot(xtrain, xtest, y_train, y_test, lr, poly_transform, axes_names):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "    \n",
    "    \n",
    "    #training data \n",
    "    #testing data \n",
    "    # lr:  linear regression object \n",
    "    #poly_transform:  polynomial transformation object \n",
    " \n",
    "    xmax=max([xtrain.values.max(), xtest.values.max()])\n",
    "\n",
    "    xmin=min([xtrain.values.min(), xtest.values.min()])\n",
    "\n",
    "    x=np.arange(xmin, xmax, 0.1)\n",
    "\n",
    "\n",
    "    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n",
    "    plt.plot(xtest, y_test, 'go', label='Test Data')\n",
    "    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n",
    "#     plt.ylim([-10000, 60000])\n",
    "    plt.xlabel(f\"{convert_param_to_label(axes_names[0])}\")\n",
    "    plt.ylabel(f\"{convert_param_to_label(axes_names[1])}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_raw_df_and_return_split(df, indep_vars, dep_var, data_threshold = 0.001):\n",
    "    \n",
    "    def df_with_numeric_values(param_cols_to_apply_condition, min_value, df):\n",
    "        return df[df[param_cols_to_apply_condition].apply(lambda x: x.ge(min_value) & x.apply(lambda y: isinstance(y, (int, float)) and not np.isnan(y)), axis=1)]\n",
    "    \n",
    "    params_of_interest = indep_vars + [dep_var]\n",
    "    filtered_data = df[params_of_interest].copy().astype('float')\n",
    "    filtered_data = df_with_numeric_values(params_of_interest, data_threshold, filtered_data).dropna()\n",
    "    \n",
    "    return filtered_data[indep_vars], filtered_data[dep_var]\n",
    "\n",
    "def build_data_to_model(input_atts: list, output_att: str, data: pd.DataFrame):\n",
    "    def df_with_numeric_values(param_cols_to_apply_condition, min_value, df):\n",
    "        return df[df[param_cols_to_apply_condition].apply(lambda x: x.ge(min_value) & x.apply(lambda y: isinstance(y, (int, float)) and not np.isnan(y)), axis=1)]\n",
    "    \n",
    "    params_of_interest = input_atts + [output_att]\n",
    "    filtered_data = data[params_of_interest].copy().astype('float')\n",
    "    filtered_data = df_with_numeric_values(params_of_interest, 0, filtered_data).dropna()\n",
    "    \n",
    "    y_data = filtered_data[output_att]\n",
    "    x_data = filtered_data.drop(output_att,axis=1)\n",
    "\n",
    "    return x_data, y_data        \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#     print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "#     print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "attributes = [x for x in grab_fpl_stats_col_names(updated_ml_data, col_filter='known') if output_attribute not in x and x not in ['opponent_team', 'team']]\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, updated_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.4, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing single param using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_in = \"team_ppda_att\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre=LinearRegression()\n",
    "lre.fit(x_train[[attribute_in]], y_train)\n",
    "print(lre.score(x_test[[attribute_in]], y_test))\n",
    "print(lre.score(x_train[[attribute_in]], y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting up into training and testing datasets are by design the first portion to train and the last portion to test. Cross-validation uses folds to split up this proportion across the entire dataset and then averages to determine a better generalized view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_stats(x_data, y_data, input_att, folds):\n",
    "    Rcross = cross_val_score(lre, x_data[[attribute_in]], y_data, cv=folds)\n",
    "    print(f\"R_cross: {Rcross}\\n\")\n",
    "    print(f\"Folds_mean: {Rcross.mean()} | Folds_std_dev: {Rcross.std()}\\n\")\n",
    "    -1 * cross_val_score(lre,x_data[[attribute_in]], y_data, cv=folds, scoring='neg_mean_squared_error')\n",
    "    yhat = cross_val_predict(lre,x_data[[attribute_in]], y_data,cv=4)\n",
    "    print(f\"Predictions for input_att for [0:5]: {yhat[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_in = \"team_ppda_att\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_stats(pd.concat([x_train, x_test]), pd.concat([y_train, y_test]), attribute_in, folds = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "attributes = [x for x in grab_fpl_stats_col_names(updated_ml_data, col_filter='known') if output_attribute not in x]\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, updated_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_of_interest = ['ict_index', 'expected_goal_involvements','bonus']\n",
    "# filtered_raw_elements = raw_elements[params_of_interest].copy().astype('float')\n",
    "# filtered_raw_elements = df_with_numeric_values(params_of_interest, 0.1, filtered_raw_elements).dropna()\n",
    "\n",
    "# attribute_out = \"bonus\"\n",
    "\n",
    "# reg_df = filtered_raw_elements[params_of_interest].copy().astype('float')\n",
    "\n",
    "# y_data = reg_df[attribute_out]\n",
    "# x_data = reg_df.drop(attribute_out,axis=1)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train[x_data.columns.tolist()], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = lr.predict(x_train[x_data.columns.tolist()])\n",
    "yhat_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = lr.predict(x_test[x_data.columns.tolist()])\n",
    "yhat_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\n",
    "DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n",
    "DistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "attributes = [x for x in grab_fpl_stats_col_names(updated_ml_data, col_filter='known') if output_attribute not in x]\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, updated_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attribute_in = \"bps\"\n",
    "\n",
    "pr = PolynomialFeatures(degree=6)\n",
    "x_train_pr = pr.fit_transform(x_train[[attribute_in]])\n",
    "x_test_pr = pr.fit_transform(x_test[[attribute_in]])\n",
    "\n",
    "poly = LinearRegression()\n",
    "poly.fit(x_train_pr, y_train)\n",
    "yhat = poly.predict(x_test_pr)\n",
    "PollyPlot(x_train[attribute_in], x_test[attribute_in], y_train, y_test, poly,pr, [attribute_in, output_attribute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = poly.predict(x_test_pr)\n",
    "# yhat[0:15]\n",
    "print(\"Predicted values:\", yhat[0:6])\n",
    "print(\"True values:\", y_test[0:6].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poly.score(x_train_pr, y_train))\n",
    "print(poly.score(x_test_pr, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_R2_across_orders(x_train, x_test, y_train, y_test, input_atts: list = None):\n",
    "    \n",
    "    if input_atts is None:\n",
    "        input_atts = pd.concat([x_train, x_test]).columns\n",
    "    \n",
    "    R2_order_pairs = []\n",
    "    iter_order = np.arange(1,13)\n",
    "    for n in iter_order:\n",
    "        pr = PolynomialFeatures(degree=n)\n",
    "\n",
    "        x_train_pr = pr.fit_transform(x_train[input_atts])\n",
    "\n",
    "        x_test_pr = pr.fit_transform(x_test[input_atts])    \n",
    "\n",
    "        lr.fit(x_train_pr, y_train)\n",
    "        R2 = lr.score(x_test_pr, y_test)\n",
    "        if R2 > 0 or n == 1:\n",
    "            R2_order_pairs.append((R2, n))\n",
    "    R2s = [x[0] for x in R2_order_pairs]\n",
    "    orders = [x[1] for x in R2_order_pairs]\n",
    "    plt.plot(orders, R2s)\n",
    "    plt.xlabel('order')\n",
    "    plt.ylabel('R^2')\n",
    "    plt.title('R^2 Using Test Data')\n",
    "evaluate_R2_across_orders(x_train, x_test, y_train, y_test, [attribute_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "attributes = [x for x in grab_fpl_stats_col_names(filtered_ml_data) if output_attribute not in x]\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, filtered_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(order, test_data, attribute_in):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n",
    "    pr = PolynomialFeatures(degree=order)\n",
    "    x_train_pr = pr.fit_transform(x_train[[attribute_in]])\n",
    "    x_test_pr = pr.fit_transform(x_test[[attribute_in]])\n",
    "    poly = LinearRegression()\n",
    "    poly.fit(x_train_pr,y_train)\n",
    "    PollyPlot(x_train[attribute_in], x_test[attribute_in], y_train, y_test, poly,pr, [attribute_in, output_attribute])\n",
    "    \n",
    "x_data=pd.concat([x_test, x_train])\n",
    "y_data=pd.concat([y_test, y_train])\n",
    "interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05), attribute_in=\"ict_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'total_points'\n",
    "attributes = [x for x in grab_fpl_stats_col_names(filtered_ml_data) if output_attribute not in x]\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, filtered_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.4, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_R2_across_orders(x_train, x_test, y_train, y_test, ['ict_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=PolynomialFeatures(degree=3)\n",
    "x_train_pr=pr.fit_transform(x_train[x_data.columns.tolist()])\n",
    "x_test_pr=pr.fit_transform(x_test[x_data.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RigeModel=Ridge(alpha=1)\n",
    "RigeModel.fit(x_train_pr, y_train)\n",
    "yhat = RigeModel.predict(x_test_pr)\n",
    "print('predicted:', yhat[0:4])\n",
    "print('test set :', y_test[0:4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "Rsqu_train = []\n",
    "dummy1 = []\n",
    "Alpha = 10 * np.array(range(0,100))\n",
    "pbar = tqdm_notebook(Alpha)\n",
    "\n",
    "for alpha in pbar:\n",
    "    RigeModel = Ridge(alpha=alpha) \n",
    "    RigeModel.fit(x_train_pr, y_train)\n",
    "    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n",
    "    \n",
    "    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n",
    "\n",
    "    Rsqu_test.append(test_score)\n",
    "    Rsqu_train.append(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(Alpha,Rsqu_test, label='validation data  ')\n",
    "plt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to find best hyperparameters used for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "attributes = [x for x in grab_fpl_stats_col_names(filtered_ml_data) if output_attribute not in x]\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, filtered_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.4, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==0.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000, 10000000]}]\n",
    "RR=Ridge()\n",
    "Grid1 = GridSearchCV(RR, parameters1,cv=4)\n",
    "Grid1.fit(x_data[x_data.columns.tolist()], y_data)\n",
    "BestRR=Grid1.best_estimator_\n",
    "BestRR.score(x_test[x_data.columns.tolist()], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important parameters of `DecisionTreeRegressor` are\n",
    "\n",
    "`criterion`: {\"mse\", \"friedman_mse\", \"mae\", \"poisson\"} - The function used to measure error\n",
    "\n",
    "`max_depth` - The max depth the tree can be\n",
    "\n",
    "`min_samples_split` - The minimum number of samples required to split a node\n",
    "\n",
    "`min_samples_leaf` - The minimum number of samples that a leaf can contain\n",
    "\n",
    "`max_features`: {\"auto\", \"sqrt\", \"log2\"} - The number of feature we examine looking for the best one, used to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_fpl_stats_col_names([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "# attributes = [x for x in [x for x in grab_fpl_stats_col_names([3, 4]) if 'expected' in x] if output_attribute not in x]\n",
    "attributes = ['ict_index', 'expected_goal_involvements']\n",
    "\n",
    "filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, filtered_raw_elements)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_tree = DecisionTreeRegressor(criterion = 'mse', max_depth=4, min_samples_split=5, min_samples_leaf=5)\n",
    "regression_tree.fit(x_train, y_train)\n",
    "regression_tree.score(x_test, y_test)\n",
    "prediction = regression_tree.predict(x_test)\n",
    "\n",
    "print((prediction - y_test).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(100,20))\n",
    "plot_tree(regression_tree, filled=True, rounded=True, feature_names=x_train.columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "try:\n",
    "    from sklearn.metrics import jaccard_score\n",
    "except:\n",
    "    from sklearn.metrics import jaccard_similarity_score as jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'event_points'\n",
    "attributes = [x for x in ['influence', 'creativity', 'threat', 'expected_goal_involvements','bps'] if output_attribute not in x]\n",
    "# attributes = [x for x in grab_fpl_stats_col_names([3, 4]) if output_attribute not in x]\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, filtered_raw_elements)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearReg = LinearRegression().fit(x_train, y_train)\n",
    "predictions = LinearReg.predict(x_test)\n",
    "\n",
    "LinearRegression_MAE = mean_absolute_error(y_test, predictions)\n",
    "LinearRegression_MSE = mean_squared_error(y_test, predictions)\n",
    "LinearRegression_R2 = r2_score(y_test, predictions)\n",
    "\n",
    "data = {\n",
    "    'LinearRegression': [LinearRegression_MAE, LinearRegression_MSE, LinearRegression_R2],\n",
    "}\n",
    "\n",
    "indices = ['MAE', 'MSE', 'R2']\n",
    "\n",
    "Report = pd.DataFrame(data, index=indices)\n",
    "Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing different models for CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN = KNeighborsClassifier(n_neighbors = 4).fit(x_train, y_train)\n",
    "# predictions = KNN.predict(x_test)\n",
    "# KNN_Accuracy_Score = accuracy_score(y_test, predictions)\n",
    "# KNN_JaccardIndex = jaccard_score(y_test, predictions)\n",
    "# KNN_F1_Score = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# Tree = DecisionTreeClassifier().fit(x_train,y_train)\n",
    "# predictions = Tree.predict(x_test)\n",
    "# Tree_Accuracy_Score = accuracy_score(y_test, predictions)\n",
    "# Tree_JaccardIndex = jaccard_score(y_test, predictions)\n",
    "# Tree_F1_Score = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "# LR = LogisticRegression().fit(x_train, y_train)\n",
    "# predictions = LR.predict(x_test)\n",
    "# predict_proba = LR.predict_proba(x_test)\n",
    "\n",
    "# LR_Accuracy_Score = accuracy_score(y_test, predictions)\n",
    "# LR_JaccardIndex = jaccard_score(y_test, predictions)\n",
    "# LR_F1_Score = f1_score(y_test, predictions, average='weighted')\n",
    "# # LR_Log_Loss = log_loss(y_test, predict_proba)\n",
    "# LR_Log_Loss = None\n",
    "\n",
    "# SVM = svm.SVC().fit(x_train, y_train)\n",
    "# predictions = SVM.predict(x_test)\n",
    "# SVM_Accuracy_Score = accuracy_score(y_test, predictions)\n",
    "# SVM_JaccardIndex = jaccard_score(y_test, predictions)\n",
    "# SVM_F1_Score = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# data = {\n",
    "#     'KNN': [KNN_Accuracy_Score, KNN_JaccardIndex, KNN_F1_Score, None],\n",
    "#     'Tree': [Tree_Accuracy_Score, Tree_JaccardIndex, Tree_F1_Score, None],\n",
    "#     'LR': [LR_Accuracy_Score, LR_JaccardIndex, LR_F1_Score, LR_Log_Loss],\n",
    "#     'SVM': [SVM_Accuracy_Score, SVM_JaccardIndex, SVM_F1_Score, None]\n",
    "# }\n",
    "\n",
    "# indices = ['Accuracy', 'Jaccard Index', 'F1 Score', 'LogLoss']\n",
    "\n",
    "# Report = pd.DataFrame(data, index=indices)\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'total_points'\n",
    "# attributes = [x for x in ['influence', 'creativity', 'threat', 'expected_goal_involvements','bps'] if output_attribute not in x]\n",
    "attributes = [x for x in grab_fpl_stats_col_names(filtered_ml_data) if output_attribute not in x]\n",
    "attributes\n",
    "\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_data, y_data = build_data_to_model(attributes, output_attribute, filtered_ml_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=1)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the points scored on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate: Controls the step size shrinkage used in each boosting round. Lower values make the model more robust but require more boosting rounds.\n",
    "\n",
    "max_depth: Maximum depth of a tree. Deeper trees can model more complex relationships but are more prone to overfitting.\n",
    "\n",
    "n_estimators: Number of boosting rounds (trees) to build. Higher values can lead to overfitting, so it's important to tune this parameter carefully.\n",
    "\n",
    "subsample: Subsample ratio of the training instances. Lower values can prevent overfitting by introducing randomness.\n",
    "\n",
    "colsample_bytree: Subsample ratio of columns when constructing each tree. Similar to subsample, but for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new XGBoost regressor with the best hyperparameters\n",
    "best_xg_reg = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the points scored on the test set\n",
    "y_pred = best_xg_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Best RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Points')\n",
    "plt.ylabel('Predicted Points')\n",
    "plt.title('Actual vs Predicted Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = ['influence', 'creativity', 'threat', 'expected_goal_involvements','bps', 'total_points']\n",
    "# attributes = [x for x in grab_fpl_stats_col_names([3, 4]) if output_attribute not in x]\n",
    "\n",
    "#Grab MID and FWDs\n",
    "# filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])][all_attributes]\n",
    "filtered_raw_elements = filtered_ml_data[all_attributes]\n",
    "#Convert teams to numerical data\n",
    "# filtered_raw_elements = pd.get_dummies(data=filtered_raw_elements, columns=['team'])\n",
    "# Create a new categorical variable based on points scored ranges\n",
    "filtered_raw_elements['points_category'] = pd.cut(filtered_raw_elements['total_points'], bins=[-np.inf, 6, 10, np.inf], labels=['bad', 'good', 'above_expectation'])\n",
    "X = filtered_raw_elements.drop(['total_points', 'points_category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "filtered_raw_elements['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Check if the clusters align with the predefined categories\n",
    "cluster_mapping = {\n",
    "    0: 'bad',\n",
    "    1: 'good',\n",
    "    2: 'above_expectation'\n",
    "}\n",
    "filtered_raw_elements['cluster_category'] = filtered_raw_elements['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Evaluate the clusters\n",
    "accuracy = (filtered_raw_elements['points_category'] == filtered_raw_elements['cluster_category']).mean()\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Initialize an empty list to store inertia values\n",
    "inertia = []\n",
    "\n",
    "# Test different numbers of clusters\n",
    "for n_clusters in range(2, 21):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the inertia values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 21), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters based on the elbow method\n",
    "optimal_n_clusters = 3  # Change this based on the plot\n",
    "\n",
    "# Perform clustering with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=0)\n",
    "filtered_raw_elements['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize the clustering\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(optimal_n_clusters):\n",
    "    cluster_data = filtered_raw_elements[filtered_raw_elements['cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['bps'], cluster_data['total_points'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Metric 1')\n",
    "plt.ylabel('Metric 2')\n",
    "plt.title('Clustering Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = filtered_raw_elements.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(24, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Remove the target variable and clustering columns from the DataFrame\n",
    "X_corr = filtered_raw_elements.drop(['total_points', 'cluster', 'points_category', 'cluster_category'], axis=1).astype(float)\n",
    "\n",
    "# Plot the clustermap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.clustermap(X_corr, cmap='coolwarm', standard_scale=1)\n",
    "plt.title('Clustermap of Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw_elements = filtered_ml_data.copy()\n",
    "filtered_raw_elements = filtered_raw_elements.drop(['minutes', 'assists', 'bonus','bps', 'influence','creativity', 'threat','ict_index', 'starts', 'expected_goals', 'expected_assists', 'expected_goal_involvements'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw_elements['points_category'] = pd.cut(filtered_raw_elements['total_points'], bins=[-np.inf, 4, 7, 9, np.inf], labels=['bad', 'good', 'great', 'above_expectation'])\n",
    "X = filtered_raw_elements.drop(['total_points', 'points_category'], axis=1)\n",
    "y_data = filtered_raw_elements['points_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "print(\"[0] number of test samples :\", x_test.shape[0])\n",
    "print(\"[0] number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Decode the predicted values\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_optimum_indep_variables(input_atts: list, output_att: str, data: pd.DataFrame):\n",
    "\n",
    "    # Define the model\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Define the list of metrics (independent variables)\n",
    "    metrics = input_atts  # Replace ... with your actual metrics\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    best_features = None\n",
    "    \n",
    "    summed_metrics = []\n",
    "    # Iterate over all possible feature combinations\n",
    "    for r in tqdm(range(1, len(metrics)+1)):\n",
    "        for feature_combination in combinations(metrics, r):\n",
    "            X_subset, y = filter_raw_df_and_return_split(data, input_atts, output_att)\n",
    "            scores = -1 * cross_val_score(rf, X_subset, y, cv=3, scoring='neg_mean_squared_error')\n",
    "            mean_score = scores.mean()\n",
    "#             print(f\"{feature_combination}: {mean_score}\")\n",
    "            summed_metrics.append((feature_combination, mean_score))\n",
    "#             if mean_score > best_score:\n",
    "#                 best_score = mean_score\n",
    "#                 best_features = feature_combination\n",
    "\n",
    "#     print(\"Best Score:\", best_score)\n",
    "#     print(\"Best Features:\", best_features)\n",
    "    return summed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in grab_fpl_stats_col_names([3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_attribute = 'event_points'\n",
    "input_atts = [x for x in ['team', 'bps', 'expected_goal_involvements', 'influence', 'creativity', 'threat'] if output_attribute not in x]\n",
    "summed_metrics = loop_optimum_indep_variables(input_atts, output_attribute, raw_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attribute = 'bps'\n",
    "# attributes = [x for x in [x for x in grab_fpl_stats_col_names([3, 4]) if 'expected' in x] if output_attribute not in x]\n",
    "attributes = ['team']\n",
    "\n",
    "filtered_raw_elements = raw_elements.loc[raw_elements['element_type'].isin([3,4])]\n",
    "x_train, x_test, y_train, y_test = build_model_sets(attributes, output_attribute, raw_elements, test_size = 0.45, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mlr(attributes, output_attribute, raw_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_rf.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = data_compiler.total_summary\n",
    "[col for col in df.columns if df[col].apply(type).eq(list).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_offset = 1\n",
    "evaluation_param = 'ict_index'\n",
    "sample_size = 20\n",
    "\n",
    "replacement_options = [x['id'] for x in data_analytics.replacement_players]\n",
    "beacon_picks = list(data_analytics.beacon_effective_ownership.keys())\n",
    "combined_prospects = list(set(replacement_options).union(set(beacon_picks)))\n",
    "\n",
    "tab_data=[]\n",
    "for player_id in combined_prospects:\n",
    "    df_index = data_compiler.total_summary.loc[data_compiler.total_summary['id_player'] == player_id].index.values[0]\n",
    "    df_sliced = data_compiler.total_summary.iloc[df_index]\n",
    "    # Create sample data\n",
    "    df = pd.DataFrame({'player_id': [df_sliced['id_player']]*len(df_sliced['round'][:len(df_sliced['round'])-gw_offset*bool(gw_offset)]),\n",
    "                            'game_week': df_sliced['round'][:len(df_sliced['round'])-gw_offset*bool(gw_offset)],\n",
    "                            'was_home': df_sliced['was_home'][:len(df_sliced['was_home'])-gw_offset*bool(gw_offset)],\n",
    "#                             'opponent_team': DataTransformer.all_df.iloc[DF_INDEX]['opponent_team'],\n",
    "                            'fdr': [helper_fns.team_rank(x) for x in df_sliced['opponent_team']][:len(df_sliced['opponent_team'])-gw_offset*bool(gw_offset)],\n",
    "                            evaluation_param: df_sliced[evaluation_param][:len(df_sliced[evaluation_param])-gw_offset*bool(gw_offset)]})\n",
    "    # split the data into training and testing sets\n",
    "    prev_score = 0\n",
    "    outputscore, outputmodel, outputsize, outputrandstate = 0, 0, 0, 0\n",
    "    for test_size in [0.2,0.3]:\n",
    "        for random_state in range(10,90,1):\n",
    "            try:\n",
    "                train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "                # define the features and target variable\n",
    "                X_train = train_df.drop(['player_id', 'game_week', evaluation_param], axis=1)\n",
    "                y_train = train_df[evaluation_param]\n",
    "                # print(X_train)\n",
    "                # print(y_train)\n",
    "                # train a linear regression model\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # evaluate the model on the test set\n",
    "                X_test = test_df.drop(['player_id', 'game_week', evaluation_param], axis=1)\n",
    "                y_test = test_df[evaluation_param]\n",
    "                score = model.score(X_test, y_test)\n",
    "                if score > 0 and score > prev_score:\n",
    "                    prev_score = score\n",
    "                    outputscore = score\n",
    "                    outputmodel = model\n",
    "                    outputsize = test_size\n",
    "                    outputrandstate = random_state\n",
    "            except Exception as e: pass\n",
    "#     if outputscore > 0.5 and outputscore < 1:\n",
    "    if outputscore > 0 and outputscore < 1:\n",
    "#         print(\"\\n\")\n",
    "        player_name = helper_fns.grab_player_name(player_id)\n",
    "#         print(player_name)\n",
    "#         print(\"\\n\")\n",
    "        # TIME TO PREDICT\n",
    "        team_id = helper_fns.grab_player_team_id(df_sliced['id_player'])\n",
    "        upcoming_fixtures = helper_fns.grab_player_fixtures('fwd',team_id,sample_size,api_ops.latest_gw-gw_offset)\n",
    "        # print(upcoming_fixtures)\n",
    "        game_week, was_home, opponent_team, fdr = [], [], [], []\n",
    "        for fixture_tup in upcoming_fixtures:\n",
    "            gw, fixtures = fixture_tup\n",
    "            for fixture in fixtures:\n",
    "                opponent_id, opponent_name, loc_val, fdr_val = fixture\n",
    "                game_week.append(gw)\n",
    "                if loc_val == 'H':\n",
    "                    was_home.append(1)\n",
    "                elif loc_val == 'A':\n",
    "                    was_home.append(0)\n",
    "                opponent_team.append(opponent_id)\n",
    "                fdr.append(fdr_val)\n",
    "        # use the model for predictions\n",
    "        new_data = pd.DataFrame({'player_id': [df_sliced['id_player']]*len(game_week),\n",
    "                                 'game_week': game_week,\n",
    "                                 'was_home': was_home,\n",
    "#                                  'opponent_team': opponent_team,\n",
    "                                 'fdr': fdr})\n",
    "        X_new = new_data.drop(['player_id', 'game_week'], axis=1)\n",
    "#         teams_against = [GrabFunctions.grab_3ltr_team_name(x) for x in X_new['opponent_team']]\n",
    "        predictions = outputmodel.predict(X_new)\n",
    "        # print('R-squared score:', outputscore)\n",
    "#         print(f'Model (test_size[{outputsize}] + rand_state[{outputrandstate}]) determined that {round(100*outputscore,2)} % of variance in dependent variables can be explained by {PARAM}')\n",
    "#         if outputscore < 0.75:\n",
    "#             print('*Not a great correlation, would suggest adjustments to aim above 75 %...')\n",
    "#         print(f'Predicted {PARAM}: {predictions}')\n",
    "#         print(f'Teams Against: {teams_against}')\n",
    "        prediction_tuple = (np.mean(predictions), predictions)\n",
    "        marker_dict = {'ict_index':'ict',\n",
    "                      'expected_goal_involvements':'xGI',\n",
    "                      'history':'history',\n",
    "                      'bps':'bps'}\n",
    "        tab_data.append([player_name,\n",
    "                     player_id,\n",
    "                     evaluation_param,\n",
    "                     outputsize,\n",
    "                     outputrandstate,\n",
    "                     round(100*outputscore,2),\n",
    "                     visualization_specs.compile_static_color_str(prediction_tuple,marker_dict[evaluation_param]),\n",
    "                     visualization_specs.get_colored_fixtures(helper_fns.grab_player_team_id(player_id),sample_size,api_ops.latest_gw-gw_offset)])\n",
    "df = pd.DataFrame(tab_data, columns = ['Player','ID','Param','test_size','random_state','Model Score',f'Predicted {evaluation_param}','Upcoming Fixtures'])\n",
    "df = df.sort_values(by=['Model Score'], ascending=False)\n",
    "table = PrettyTable()\n",
    "table.field_names = df.columns\n",
    "for row in df.values:\n",
    "    table.add_row(row)\n",
    "table.align[f'Predicted {evaluation_param}'] = \"l\"\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
